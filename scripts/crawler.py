import asyncio
import aiohttp
import aiofiles
from bs4 import BeautifulSoup
import json
import time
import random

# ... parse_ytower_recipe_content å‡½å¼ä¿æŒä¸è®Šï¼Œå®ƒå·²ç¶“è¢«é©—è­‰æ˜¯æ­£ç¢ºçš„ ...
def parse_ytower_recipe_content(soup: BeautifulSoup, url: str) -> dict:
    # (æ­¤è™•çœç•¥ï¼Œè«‹æ²¿ç”¨ä¸Šä¸€ç‰ˆçš„ç¨‹å¼ç¢¼)
    recipe_data = {
        "name": None, "image_url": None, "core_ingredients": [],
        "full_ingredient_list": {}, "steps": [], "total_time": None,
        "difficulty": None, "cuisine_style": None, "servings": None,
        "key_equipment": None, "tips": None, "nutrition_info": None,
    }
    title_tag = soup.find('div', id='recipe_name')
    if title_tag:
        recipe_data['name'] = title_tag.get_text(strip=True)
    image_meta_tag = soup.find('meta', property='og:image')
    if image_meta_tag:
        recipe_data['image_url'] = image_meta_tag.get('content')
    ingredient_uls = soup.find_all('ul', class_='ingredient')
    for ul in ingredient_uls:
        group_title_tag = ul.find('li')
        if not group_title_tag: continue
        group_title = group_title_tag.get_text(strip=True)
        items = ul.find_all('span', class_='ingredient_name')
        group_ingredients = {}
        for item in items:
            name_tag = item.find('a')
            amount_tag = item.find('span', class_='ingredient_amount')
            name = name_tag.get_text(strip=True) if name_tag else ''
            amount = amount_tag.get_text(strip=True) if amount_tag else ''
            if name:
                group_ingredients[name] = amount
        recipe_data['full_ingredient_list'][group_title] = group_ingredients
        if 'ã€æã€€æ–™ã€‘' in group_title:
            recipe_data['core_ingredients'].extend(group_ingredients.keys())
    step_tags = soup.find_all('li', class_='step')
    recipe_data['steps'] = [step.get_text(separator='\n', strip=True) for step in step_tags]
    return recipe_data


async def fetch_and_parse(session: aiohttp.ClientSession, url: str, semaphore: asyncio.Semaphore, retries=2):
    async with semaphore:
        await asyncio.sleep(random.uniform(2.0, 5.0)) # æ‹‰é•·ä¸¦æ‹‰å¤§éš¨æ©Ÿå»¶é²ç¯„åœ
        for attempt in range(retries):
            try:
                async with session.get(url, timeout=30) as response:
                    if response.status >= 400: # æ•æ‰æ‰€æœ‰å®¢æˆ¶ç«¯å’Œä¼ºæœå™¨éŒ¯èª¤
                        print(f"ğŸŸ¡ è«‹æ±‚ç•°å¸¸ (ç‹€æ…‹ç¢¼: {response.status}) æ–¼ {url}, æº–å‚™é‡è©¦ (ç¬¬ {attempt + 1}/{retries} æ¬¡)")
                        await asyncio.sleep(5 * (attempt + 1))
                        continue
                    
                    html_content = await response.text(encoding='big5-hkscs', errors='ignore')
                    soup = BeautifulSoup(html_content, 'html.parser')
                    data = await asyncio.to_thread(parse_ytower_recipe_content, soup, url)
                    
                    if not data or not data.get('name'):
                        # å³ä½¿ç‹€æ…‹ç¢¼200ï¼Œä¹Ÿå¯èƒ½å›å‚³ç„¡æ•ˆé é¢ï¼Œéœ€å¢åŠ åˆ¤æ–·
                        print(f"âš ï¸ å…§å®¹è§£æç‚ºç©º: {url}")
                        return None

                    print('.', end='', flush=True)
                    return data
            except Exception as e:
                print(f"âŒ è™•ç†æ™‚ç™¼ç”ŸéŒ¯èª¤: {url}, éŒ¯èª¤: {e}")
                if attempt == retries - 1: return None
        return None

async def main():
    urls_file = 'unique_urls.txt'
    try:
        async with aiofiles.open(urls_file, mode='r', encoding='utf-8') as f:
            target_urls = [line.strip() for line in await f.readlines() if line.strip()]
    except FileNotFoundError:
        print(f"éŒ¯èª¤ï¼šæ‰¾ä¸åˆ°é€£çµæª”æ¡ˆ '{urls_file}'ã€‚")
        return

    print(f"å¾ {urls_file} è®€å–åˆ° {len(target_urls)} å€‹é£Ÿè­œé€£çµ...")
    
    headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36'}
    
    # --- æˆ°è¡“åƒæ•¸èª¿æ•´ ---
    BATCH_SIZE = 50  # æ¯ 50 å€‹ URL ç‚ºä¸€å€‹æ‰¹æ¬¡
    CONCURRENT_REQUESTS = 5 # éå¸¸ä¿å®ˆçš„ä¸¦ç™¼æ•¸
    
    all_recipes = []
    
    for i in range(0, len(target_urls), BATCH_SIZE):
        batch_urls = target_urls[i:i + BATCH_SIZE]
        print(f"\n--- æ­£åœ¨è™•ç†æ‰¹æ¬¡ {i // BATCH_SIZE + 1} (URL {i+1} - {i+len(batch_urls)}) ---")
        
        # ç‚ºæ¯ä¸€å€‹æ‰¹æ¬¡å»ºç«‹å…¨æ–°çš„ Session å’Œ Connector
        connector = aiohttp.TCPConnector(limit_per_host=5)
        semaphore = asyncio.Semaphore(CONCURRENT_REQUESTS)
        
        async with aiohttp.ClientSession(headers=headers, connector=connector) as session:
            tasks = [asyncio.create_task(fetch_and_parse(session, url, semaphore)) for url in batch_urls]
            results = await asyncio.gather(*tasks)
            
            all_recipes.extend([res for res in results if res])
            
        # ä¸€å€‹æ‰¹æ¬¡çµæŸå¾Œï¼Œéš¨æ©Ÿä¼‘æ¯ 8 åˆ° 15 ç§’
        sleep_duration = random.uniform(8, 15)
        print(f"\næ‰¹æ¬¡å®Œæˆï¼Œéš¨æ©Ÿä¼‘æ¯ {sleep_duration:.2f} ç§’...")
        await asyncio.sleep(sleep_duration)

    output_filename = 'recipes_data.json'
    async with aiofiles.open(output_filename, 'w', encoding='utf-8') as f:
        await f.write(json.dumps(all_recipes, ensure_ascii=False, indent=4))

    print(f"\nå…¨éƒ¨çˆ¬å–å®Œæˆï¼å…± {len(target_urls)} å€‹é€£çµï¼ŒæˆåŠŸè§£æ {len(all_recipes)} ç­†é£Ÿè­œã€‚")
    print(f"çµæœå·²å„²å­˜è‡³ {output_filename}")


if __name__ == "__main__":
    start_time = time.time()
    asyncio.run(main())
    end_time = time.time()
    print(f"ç¸½è€—æ™‚: {end_time - start_time:.2f} ç§’")